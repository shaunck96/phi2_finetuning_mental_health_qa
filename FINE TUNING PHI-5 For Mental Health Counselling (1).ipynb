{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c19e9f6-c7c6-418d-89ec-76cde77935d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tuning transfomers version == 4.31.0\n",
    "#PEFT - Parameter efficient\n",
    "#torch - torch backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4456f8a-770f-4b7d-a3db-bfc39c61c2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: q in /usr/local/lib/python3.10/dist-packages (2.7)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: peft==0.4.0 in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
      "Requirement already satisfied: transformers==4.31.0 in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
      "Requirement already satisfied: trl==0.4.7 in /usr/local/lib/python3.10/dist-packages (0.4.7)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.26.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (6.0.1)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (0.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.20.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.13.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl==0.4.7) (2.16.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.7) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.7) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.7) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.7) (2.1.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.7) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.7) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl==0.4.7) (3.9.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.7) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.7) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.7) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.7) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.7) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl==0.4.7) (4.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.4.7) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.4.7) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl==0.4.7) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.4.7) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: bitsandbytes-cuda110 in /usr/local/lib/python3.10/dist-packages (0.26.0.post2)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n",
      "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->bitsandbytes) (1.26.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install q torch peft==0.4.0 transformers==4.31.0 trl==0.4.7 accelerate einops tqdm scipy\n",
    "!pip install bitsandbytes-cuda110 bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c703427b-fa53-439f-865d-1a70befdad0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Token:  ········\n",
      "Add token as git credential? (Y/n)  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "AutoModelForCausalLM,\n",
    "AutoTokenizer,\n",
    "BitsAndBytesConfig,\n",
    "HfArgumentParser,\n",
    "TrainingArguments\n",
    ") #Text generation task\n",
    "from tqdm.notebook import tqdm #progress bard\n",
    "from trl import SFTTrainer #SFT Training\n",
    "from huggingface_hub import interpreter_login\n",
    "import pandas as pd\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af09bcb-032d-4f9e-9355-c97e4eb51d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Context', 'Response'],\n",
       "    num_rows: 3512\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading Mental Health DataSet - load dataset with question answers about mental health topics\n",
    "dataset = load_dataset(\"Amod/mental_health_counseling_conversations\", split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1781fbb3-ba97-4142-93f2-9caacdb7627f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>If everyone thinks you're worthless, then mayb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Hello, and thank you for your question and see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>First thing I'd suggest is getting the sleep y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm going through some things with my feelings...</td>\n",
       "      <td>Therapy is essential for those that are feelin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Context  \\\n",
       "0  I'm going through some things with my feelings...   \n",
       "1  I'm going through some things with my feelings...   \n",
       "2  I'm going through some things with my feelings...   \n",
       "3  I'm going through some things with my feelings...   \n",
       "\n",
       "                                            Response  \n",
       "0  If everyone thinks you're worthless, then mayb...  \n",
       "1  Hello, and thank you for your question and see...  \n",
       "2  First thing I'd suggest is getting the sleep y...  \n",
       "3  Therapy is essential for those that are feelin...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert to a csv\n",
    "df = pd.DataFrame(dataset)\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc121e0-73e9-4353-9c30-73963a8ee220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[INST] I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\\n   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\\n   How can I change my feeling of being worthless to everyone? [/INST] If everyone thinks you're worthless, then maybe you need to find new people to hang out with.Seriously, the social context in which a person lives is a big influence in self-esteem.Otherwise, you can go round and round trying to understand why you're not worthless, then go back to the same crowd and be knocked down again.There are many inspirational messages you can find in social media. \\xa0Maybe read some of the ones which state that no person is worthless, and that everyone has a good purpose to their life.Also, since our culture is so saturated with the belief that if someone doesn't feel good about themselves that this is somehow terrible.Bad feelings are part of living. \\xa0They are the motivation to remove ourselves from situations and relationships which do us more harm than good.Bad feelings do feel terrible. \\xa0 Your feeling of worthlessness may be good in the sense of motivating you to find out that you are much better than your feelings today. \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Note - to fine tune for a different language, create a vocab of dataset using sentenpiece, pass it to the tokenizer, fine tune\n",
    "def format_row(row):\n",
    "    question = row['Context']\n",
    "    answer = row['Response']\n",
    "    formatted_string = f\"[INST] {question} [/INST] {answer} \" #format used for llama\n",
    "    return formatted_string\n",
    "\n",
    "df['Formatted'] = df.apply(format_row, axis=1)\n",
    "new_df = df.rename(columns = {'Formatted':'Text'})\n",
    "new_df = new_df[['Text']]\n",
    "new_df['Text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8739cf7-ed8d-48dc-abe2-5b52bfff33c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"formatted_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88eff21b-9cda-406e-be37-e4d0ebf6bfda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db186cee6c9f422bab6f8b1bee4b8508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Text'],\n",
       "    num_rows: 3512\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = load_dataset(\"csv\", data_files=\"formatted_data.csv\", split=\"train\")\n",
    "training_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24d137c5-82a6-4af5-a5f0-11295f2801d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Token:  ········\n",
      "Add token as git credential? (Y/n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc968c28-e96a-4059-b4ca-4affcbdf7ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RepoUrl('https://huggingface.co/shaunck96/mhGPT', endpoint='https://huggingface.co', repo_type='model', repo_id='shaunck96/mhGPT')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import create_repo\n",
    "create_repo(\"shaunck96/mhGPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89fd65f7-c214-477c-a321-a9d89e7602f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Token:  ········\n",
      "Add token as git credential? (Y/n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75345b3a-323f-4ee6-9b01-33260a1f92f2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7514b62b2e6a4227a502a3a790f8fe2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240111_183848-66ssjjj2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shaunshib96/huggingface/runs/66ssjjj2' target=\"_blank\">avid-wave-2</a></strong> to <a href='https://wandb.ai/shaunshib96/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shaunshib96/huggingface' target=\"_blank\">https://wandb.ai/shaunshib96/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shaunshib96/huggingface/runs/66ssjjj2' target=\"_blank\">https://wandb.ai/shaunshib96/huggingface/runs/66ssjjj2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CodeGenTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [108/108 41:10, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Fine Tuning\n",
    "base_model = \"microsoft/phi-2\"\n",
    "new_model = \"phi2-mental-health\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "#QLora quantization Config (4-bit)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = \"nf4\", #default quanitzation makes it slower ; nf4 is a special datatype to speed up calculation when input sequence is too long ; helpts to seleect split marker faster than traiditonal qlora\n",
    "    bnb_4bit_compute_dypte = torch.float16,\n",
    "    bnb_4bit_use_double_quant = False\n",
    ")\n",
    "\n",
    "#phi2 doesn't support flash attention at the time of writing this notebook\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config = bnb_config,\n",
    "    trust_remote_code = True,\n",
    "    flash_attn = True,\n",
    "    flash_rotary = True,\n",
    "    fused_dense = True,\n",
    "    low_cpu_mem_usage = True,\n",
    "    device_map = 0,\n",
    "    revision = \"refs/pr/23\"\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = \"./mhGPT\",\n",
    "    num_train_epochs = 2,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 32,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    eval_steps = 1500,\n",
    "    logging_steps = 15,\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    learning_rate = 2e-4,\n",
    "    save_steps = 1500,\n",
    "    warmup_ratio = 0.05,\n",
    "    weight_decay = 0.01,\n",
    "    max_steps = -1\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r = 32,\n",
    "    lora_alpha = 64,\n",
    "    lora_dropout = 0.05,\n",
    "    bias = \"none\",\n",
    "    task_type = \"CAUSAL_LM\",\n",
    "    target_modules = [\"Wqkv\", \"fc1\", \"fc2\"]\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = training_dataset,\n",
    "    peft_config = peft_config,\n",
    "    dataset_text_field = \"Text\",\n",
    "    max_seq_length = 690,\n",
    "    tokenizer = tokenizer,\n",
    "    args = training_arguments\n",
    ")\n",
    "\n",
    "#35-40 mins for 2 epochs\n",
    "trainer.train()\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3f8c8775-ebb9-4ae1-9e8b-765d7e1d7a4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1099, in _get_module\n",
      "    SACREMOSES_IMPORT_ERROR = \"\"\"\n",
      "  File \"/usr/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\", line 32, in <module>\n",
      "    from ..models.auto.modeling_auto import AutoModelForDepthEstimation, AutoModelForImageToImage\n",
      "ImportError: cannot import name 'AutoModelForImageToImage' from 'transformers.models.auto.modeling_auto' (/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2787/3301594921.py\", line 1, in <module>\n",
      "    from transformers import pipeline\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1089, in __getattr__\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\", line 1101, in _get_module\n",
      "    `pip install sacremoses`. Please note that you may need to restart your runtime after installation.\n",
      "RuntimeError: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\n",
      "cannot import name 'AutoModelForImageToImage' from 'transformers.models.auto.modeling_auto' (/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2142, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Run text generation pipeline with our next model\n",
    "prompt = \"I am not able to sleep in night. Do you have any suggestions?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=250)\n",
    "result = pipe(f\"[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43ed81a-423d-4d51-8894-0ee0f33a7bd9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd2db4ce-46d7-4c77-b91b-ae68602cb1e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m pipe\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m trainer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgc\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "del model\n",
    "del pipe\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2485475a-af54-490d-908c-31c8dd03a8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/phi-2\"\n",
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/phi-2\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=0,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
